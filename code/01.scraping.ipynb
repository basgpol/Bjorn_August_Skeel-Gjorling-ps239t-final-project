{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection: Scraping transcripts of parliamentary debates in Denmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook consist of 5 parts representing my data collection:\n",
    "    1. Part 1 loads all the required packages\n",
    "    2. Part 2 creates a function that collects urls to transcripts of danish parliamentary debates\n",
    "    3. Part 3 creates a function that collects the title, date and content of a parliamentary debate from a url \n",
    "    4. Part 4 scrapes all the transcripts using both the url-function and the scraper-function\n",
    "    5. Part 5 saves both the urls and the transcripts using pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I load all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: URL-collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I create and test a function that collects urls to transcripts of parliamentary debates in Denmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ft_url_collect(year = None, month = None, day = None):\n",
    "    \"\"\" Function that collects url-links to transcript of danish parliamentary debates\n",
    "        The function takes three imports:\n",
    "        1) The year of the start date you want (e.g. \"2012\"). The default is year \"2000\".\n",
    "        2) The month written as two-digits of the start date you want (e.g. \"07\"). The default is \"01\" (january).\n",
    "        3) The day written as two-digits of the start date you want (e.g. \"31\"). The default is is \"01\"\n",
    "        \n",
    "        By 14th March 2018 the transcripts go back to 5th October 2004 \n",
    "        \"\"\"\n",
    "    \n",
    "    ### Part 1 of the function: Creating the urls ###\n",
    "    base_url = \"http://www.ft.dk/da/dokumenter/dokumentlister/referater?pageSize=200&startDate=\"\n",
    "    \n",
    "    if year == None:\n",
    "        year = \"2000\"\n",
    "    if month == None:\n",
    "        month = \"01\"\n",
    "    if day == None:\n",
    "        day = \"01\"\n",
    "    \n",
    "    startdate = str(year)+str(month)+str(day) #creating start date\n",
    "    url = base_url+startdate #creating url with links to debate transcripts\n",
    "    \n",
    "    response = requests.get(url) # GET-request\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    hits_str = soup.find(\"span\", attrs={'class':'results'}).text #collecting the number of hits in a string\n",
    "    hits = [int(s) for s in hits_str.split() if s.isdigit()][0] #convert number of hits to int\n",
    "    number_of_pages = math.ceil(hits/200) #collect number of pages (every page have 200 links)\n",
    "    pages_numbers = np.arange(number_of_pages+1)[1:] #creating np.array with a number of each of the pages\n",
    "    \n",
    "    \n",
    "    page_url_template = soup.find(\"li\", attrs={'class':'next'}).find(\"a\")[\"href\"] #find the url-template for the difference pages\n",
    "    page_urls = np.array(\"page_url\") # creating empty np.array\n",
    "    \n",
    "    for page_number in pages_numbers: #creating ulrs for each page with links\n",
    "        page_urls = np.append(page_urls, \"http://www.ft.dk/da/dokumenter/dokumentlister/referater\" + page_url_template.replace(\"pageNumber=2\",\"pageNumber=\"+str(page_number)))\n",
    "    page_urls = page_urls[1:] #deling the irrelevant first item   \n",
    "    \n",
    "    ### Part 2 of the function: Collecting the links to the debate transcripts ###\n",
    "    np_links = np.array(\"link\") # creating empty numpy array\n",
    "    \n",
    "    for page_url in page_urls:\n",
    "        response = requests.get(page_url) # GET-request\n",
    "        soup2 = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # Creating a loop that collects every link and only keep the links that contain \"forhandling\" in th url\n",
    "        for link in soup2.find_all(\"a\"):\n",
    "            every_link = link.get(\"href\")\n",
    "            if every_link[1:14] == \"forhandlinger\": \n",
    "                np_links = np.append(np_links, \"http://www.ft.dk\"+every_link)\n",
    "\n",
    "    links = np_links[1:] # drop the first irrelevant element\n",
    "    links = np.unique(links) # drop duplicates\n",
    "    \n",
    "    return (links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://www.ft.dk/forhandlinger/20041/20041_M10_helemoedet.pdf',\n",
       "       'http://www.ft.dk/forhandlinger/20041/20041_M11_helemoedet.pdf',\n",
       "       'http://www.ft.dk/forhandlinger/20041/20041_M12_helemoedet.pdf',\n",
       "       'http://www.ft.dk/forhandlinger/20041/20041_M13_helemoedet.pdf',\n",
       "       'http://www.ft.dk/forhandlinger/20041/20041_M14_helemoedet.pdf'],\n",
       "      dtype='<U66')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the collector\n",
    "ft_url_collect(year = \"1999\", month = \"07\", day = \"01\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Transcript collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I create and test a function that when receiving an url as input returns the title, date and text content of a parliamentary debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_title_date_text(debate_url):\n",
    "    \"\"\"This function takes as input an URL with the transscript of the parliamentary debate in html-format \n",
    "       and return a np.array with three elements: title of the debate, date of the debate, and a string \n",
    "       with the content of the debate\"\"\"\n",
    "    \n",
    "    response = requests.get(debate_url) # GET-request\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') #turn into a soup\n",
    "    \n",
    "    try: # Some htmls have no content due to special events e.g. election. This code ignores an error if this is the case\n",
    "         # Finding element 1: The title of the debate\n",
    "        title = soup.find(\"p\", attrs={'class':'Titel'}).text\n",
    "    \n",
    "        # Finding element 2: The date and time of the debate\n",
    "        date = soup.find(\"meta\", attrs={'name':'DateOfSitting'}).get(\"content\")\n",
    "    \n",
    "        # Finding element 3: The content of the debate (Everything that was said in the debate)\n",
    "        all_text_parts = soup.find_all(\"p\", attrs={'class':'Tekst'}) + soup.find_all(\"p\", attrs={'class':'TekstIndryk'}) #getting a list with all text parts\n",
    "        all_text = \"\" #creating a empty character string\n",
    "    \n",
    "        for text_part in all_text_parts: #creating a loop that take all text parts and collects them in one string\n",
    "            text = text_part.text\n",
    "            all_text = all_text + text + \" \"\n",
    "        all_text = all_text.replace(\"\\n\", \"\") #removing \\n\n",
    "    \n",
    "        # Collecting all elements in one np.array\n",
    "        result = [title, date, all_text] \n",
    "    \n",
    "        return(result)\n",
    "    \n",
    "    except: #Continues if an error happens\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18. møde', '2017-11-14T13:00:00']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the transcript collector function\n",
    "test_url = \"http://www.ft.dk/forhandlinger/20171/20171M018_2017-11-14_1300.htm\"\n",
    "scrape_title_date_text(test_url)[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4: Scraping transcripts for debates since 1-1-2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I use the previous created functions. I first collect urls to all debates since 1/1/2000. Thereafter, I exclude all urls that do not contain transcripts in html-format. Lastly, I use the transcript-collector function to get the title, date and text content of each url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collecting urls to all available transcripts \n",
    "all_urls = ft_url_collect(year = \"2000\", month = \"01\", day = \"01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1454"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of urls collected\n",
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperating transcripts in html and pdf\n",
    "html_urls = [url for url in all_urls if url.find(\"pdf\")==-1]\n",
    "pdf_urls = [url for url in all_urls if url.find(\"pdf\")!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1154"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of HTML urls\n",
    "len(html_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.ft.dk/forhandlinger/20071/20071M001_2007-10-02_1200.htm',\n",
       " 'http://www.ft.dk/forhandlinger/20071/20071M002_2007-10-03_1300.htm',\n",
       " 'http://www.ft.dk/forhandlinger/20071/20071M003_2007-10-04_1000.htm']"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the result of html-urls\n",
    "html_urls[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.ft.dk/forhandlinger/20041/20041_M10_helemoedet.pdf',\n",
       " 'http://www.ft.dk/forhandlinger/20041/20041_M11_helemoedet.pdf',\n",
       " 'http://www.ft.dk/forhandlinger/20041/20041_M12_helemoedet.pdf']"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the result of pdf-urls\n",
    "pdf_urls[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, I loop through all html-urls to collect transcripts of parliamentary debates in Denmark. I receive an error after 18 iterations because the webpage won't allow me to make many calls. I have randomized the time intervals between each call, in order to increase the number of calls I can make before the webpage cuts my access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1 is done\n",
      "Iteration number 2 is done\n",
      "Iteration number 3 is done\n",
      "Iteration number 4 is done\n",
      "Iteration number 5 is done\n",
      "Iteration number 6 is done\n",
      "Iteration number 7 is done\n",
      "Iteration number 8 is done\n",
      "Iteration number 9 is done\n",
      "Iteration number 10 is done\n",
      "Iteration number 11 is done\n",
      "Iteration number 12 is done\n",
      "Iteration number 13 is done\n",
      "Iteration number 14 is done\n",
      "Iteration number 15 is done\n",
      "Iteration number 16 is done\n",
      "Iteration number 17 is done\n",
      "Iteration number 18 is done\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "(\"Connection broken: ConnectionResetError(10054, 'En eksisterende forbindelse blev tvangsafbrudt af en ekstern vært', None, 10054, None)\", ConnectionResetError(10054, 'En eksisterende forbindelse blev tvangsafbrudt af en ekstern vært', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] En eksisterende forbindelse blev tvangsafbrudt af en ekstern vært",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    400\u001b[0m                         \u001b[1;31m# Content-Length are caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp_bytes_read\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength_remaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[1;31m# This includes IncompleteRead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Connection broken: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProtocolError\u001b[0m: (\"Connection broken: ConnectionResetError(10054, 'En eksisterende forbindelse blev tvangsafbrudt af en ekstern vært', None, 10054, None)\", ConnectionResetError(10054, 'En eksisterende forbindelse blev tvangsafbrudt af en ekstern vært', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-291-a1a613f7fe04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhtml_urls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdebate_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_title_date_text_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#scraping data from url using scraper-function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mdebates_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebate_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#appending scraped data to list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# waits a random nuber of seconds between 0 and 20 before next iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-233-4ea831c7178b>\u001b[0m in \u001b[0;36mscrape_title_date_text_test\u001b[1;34m(debate_url)\u001b[0m\n\u001b[0;32m      4\u001b[0m        with the content of the debate\"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebate_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# GET-request\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#turn into a soup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    506\u001b[0m         }\n\u001b[0;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    821\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    746\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mContentDecodingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: (\"Connection broken: ConnectionResetError(10054, 'En eksisterende forbindelse blev tvangsafbrudt af en ekstern vært', None, 10054, None)\", ConnectionResetError(10054, 'En eksisterende forbindelse blev tvangsafbrudt af en ekstern vært', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "# Creating a loop that scrape debate data from each url\n",
    "debates_list = [] #creating a empty list\n",
    "iteration = 1\n",
    "\n",
    "for url in html_urls:\n",
    "    debate_data = scrape_title_date_text_test(url) #scraping data from url using scraper-function\n",
    "    debates_list.append(debate_data) #appending scraped data to list\n",
    "    time.sleep(np.random.choice(20))  # waits a random number of seconds between 0 and 20 before next iteration\n",
    "    print(\"Iteration number \" + str(iteration) + \" is done\") #print which iteration that is completed\n",
    "    iteration = iteration + 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limits of the amount of calls the Danish Parliament allows us to scrape, I have added the following code to continue the scraping process, when then Danish Parliament is disconnecting my access. I run this chunk of code until all the debate transcripts have been scraped. This requires several calls of the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1104 is done\n",
      "Iteration number 1105 is done\n",
      "Iteration number 1106 is done\n",
      "Iteration number 1107 is done\n",
      "Iteration number 1108 is done\n",
      "Iteration number 1109 is done\n",
      "Iteration number 1110 is done\n",
      "Iteration number 1111 is done\n",
      "Iteration number 1112 is done\n",
      "Iteration number 1113 is done\n",
      "Iteration number 1114 is done\n",
      "Iteration number 1115 is done\n",
      "Iteration number 1116 is done\n",
      "Iteration number 1117 is done\n",
      "Iteration number 1118 is done\n",
      "Iteration number 1119 is done\n",
      "Iteration number 1120 is done\n",
      "Iteration number 1121 is done\n",
      "Iteration number 1122 is done\n",
      "Iteration number 1123 is done\n",
      "Iteration number 1124 is done\n",
      "Iteration number 1125 is done\n",
      "Iteration number 1126 is done\n",
      "Iteration number 1127 is done\n",
      "Iteration number 1128 is done\n",
      "Iteration number 1129 is done\n",
      "Iteration number 1130 is done\n",
      "Iteration number 1131 is done\n",
      "Iteration number 1132 is done\n",
      "Iteration number 1133 is done\n",
      "Iteration number 1134 is done\n",
      "Iteration number 1135 is done\n",
      "Iteration number 1136 is done\n",
      "Iteration number 1137 is done\n",
      "Iteration number 1138 is done\n",
      "Iteration number 1139 is done\n",
      "Iteration number 1140 is done\n",
      "Iteration number 1141 is done\n",
      "Iteration number 1142 is done\n",
      "Iteration number 1143 is done\n",
      "Iteration number 1144 is done\n",
      "Iteration number 1145 is done\n",
      "Iteration number 1146 is done\n",
      "Iteration number 1147 is done\n",
      "Iteration number 1148 is done\n",
      "Iteration number 1149 is done\n",
      "Iteration number 1150 is done\n",
      "Iteration number 1151 is done\n",
      "Iteration number 1152 is done\n",
      "Iteration number 1153 is done\n",
      "Iteration number 1154 is done\n"
     ]
    }
   ],
   "source": [
    "# Continuing the scraping process\n",
    "\n",
    "for url in html_urls[iteration-1:]:\n",
    "    debate_data = scrape_title_date_text(url) #scraping data from url using scraper-function\n",
    "    debates_list.append(debate_data) #appending scraped data to list\n",
    "    time.sleep(np.random.choice(20))  #waits a random nuber of seconds between 0 and 20 before next iteration\n",
    "    print(\"Iteration number \" + str(iteration) + \" is done\") #print which iteration that is completed\n",
    "    iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Saving the transcripts and urls using pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I save the data using pickle. I use pickle instead of CSV, because I have used all the RAM on my computer at this point. The pickle-function allows me to save the data without using RAM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I delete one transcripts. This transcipt was empty due to a parliamentary election. \n",
    "all_debates_clean = debates_list[:10] + debates_list[11:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save object with all debate urls\n",
    "pickle.dump(all_urls, open( \"debate_urls.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save object with all debates\n",
    "pickle.dump(all_debates_clean, open( \"debates.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the next Jupyter Notebook *(02.text_analysis.ipynb)* to see the code used for the text analysis. \n",
    "The code is also available here: https://github.com/basgpol/ps239t-final-project/blob/master/code/02.text_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
